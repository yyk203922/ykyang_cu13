{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c080995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7ff4b1df9e469a9e16304c167812ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1474 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_605331/3068749408.py:80: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7,4\"\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    ")\n",
    "import numpy as np, evaluate\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "# ============ åŸºæœ¬é…ç½® ============\n",
    "model_name = \"../models/flan-t5-base\"\n",
    "train_file = \"/data/ykyang/datasets/pdtb3_T5/pdtb3_train_instruction.json\"\n",
    "dev_file   = \"/data/ykyang/datasets/pdtb3_T5/pdtb3_dev_instruction.json\"\n",
    "test_file  = \"/data/ykyang/datasets/pdtb3_T5/pdtb3_test_instruction.json\"\n",
    "output_dir = \"../checkpoints/flan-t5-sft.json\"\n",
    "num_epochs = 6\n",
    "batch_size = 8\n",
    "\n",
    "# ============ æ¨¡å‹å’Œåˆ†è¯å™¨ ============\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# ============ æ•°æ®é›† ============\n",
    "data_files = {\"train\": train_file, \"validation\": dev_file, \"test\": test_file}\n",
    "raw_datasets = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "# ============ æ•°æ®é¢„å¤„ç† ============\n",
    "def preprocess(examples):\n",
    "    inputs = [f\"{ins}\\n\\n{inp}\" for ins, inp in zip(examples[\"instruction\"], examples[\"input\"])]\n",
    "    targets = [t.strip() for t in examples[\"output\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=256, truncation=True)\n",
    "    labels = tokenizer(targets, max_length=8, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized = raw_datasets.map(preprocess, batched=True, remove_columns=raw_datasets[\"train\"].column_names)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# ============ è¯„ä»·æŒ‡æ ‡å‡½æ•° ============\n",
    "def compute_metrics(eval_pred):\n",
    "    preds_ids, labels_ids = eval_pred\n",
    "    if isinstance(preds_ids, tuple):\n",
    "        preds_ids = preds_ids[0]\n",
    "    preds_ids = np.where(preds_ids != -100, preds_ids, tokenizer.pad_token_id)\n",
    "    labels_ids = np.where(labels_ids != -100, labels_ids, tokenizer.pad_token_id)\n",
    "    \n",
    "    preds = tokenizer.batch_decode(preds_ids, skip_special_tokens=True)\n",
    "    labels = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "    preds = [p.strip() for p in preds]\n",
    "    labels = [l.strip() for l in labels]\n",
    "    \n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1}\n",
    "\n",
    "# ============ è®­ç»ƒå‚æ•° ============\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,       # æ¯500 stepéªŒè¯\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    learning_rate=5e-5,\n",
    "    predict_with_generate=True,\n",
    "    bf16=True,    # ä½¿ç”¨ bf16\n",
    "    fp16=False,\n",
    "    logging_steps=100,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    save_total_limit=7,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# ============ Trainer ============\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d001a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ykyang/miniconda3/envs/YYK_conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='6420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 501/6420 09:27 < 1:52:12, 0.88 it/s, Epoch 0.47/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='101' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 15/101 00:13 < 01:21, 1.06 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ============ 7. è®­ç»ƒ ============\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/YYK_conda/lib/python3.11/site-packages/transformers/trainer.py:2328\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2326\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2327\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/YYK_conda/lib/python3.11/site-packages/transformers/trainer.py:2754\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2752\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.epoch = epoch + (step + \u001b[32m1\u001b[39m + steps_skipped) / steps_in_epoch\n\u001b[32m   2753\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2754\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2755\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2756\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2757\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2758\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2759\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2760\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2761\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2762\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2763\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2764\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2765\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_substep_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/YYK_conda/lib/python3.11/site-packages/transformers/trainer.py:3227\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[39m\n\u001b[32m   3225\u001b[39m metrics = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3226\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_evaluate:\n\u001b[32m-> \u001b[39m\u001b[32m3227\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3228\u001b[39m     is_new_best_metric = \u001b[38;5;28mself\u001b[39m._determine_best_metric(metrics=metrics, trial=trial)\n\u001b[32m   3230\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_strategy == SaveStrategy.BEST:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/YYK_conda/lib/python3.11/site-packages/transformers/trainer.py:3176\u001b[39m, in \u001b[36mTrainer._evaluate\u001b[39m\u001b[34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[39m\n\u001b[32m   3175\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m3176\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3177\u001b[39m     \u001b[38;5;28mself\u001b[39m._report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m.state.global_step, metrics)\n\u001b[32m   3179\u001b[39m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/YYK_conda/lib/python3.11/site-packages/transformers/trainer_seq2seq.py:191\u001b[39m, in \u001b[36mSeq2SeqTrainer.evaluate\u001b[39m\u001b[34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28mself\u001b[39m.gather_function = \u001b[38;5;28mself\u001b[39m.accelerator.gather\n\u001b[32m    190\u001b[39m \u001b[38;5;28mself\u001b[39m._gen_kwargs = gen_kwargs\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/YYK_conda/lib/python3.11/site-packages/transformers/trainer.py:4469\u001b[39m, in \u001b[36mTrainer.evaluate\u001b[39m\u001b[34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4466\u001b[39m start_time = time.time()\n\u001b[32m   4468\u001b[39m eval_loop = \u001b[38;5;28mself\u001b[39m.prediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.use_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.evaluation_loop\n\u001b[32m-> \u001b[39m\u001b[32m4469\u001b[39m output = \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4470\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4471\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEvaluation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4472\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[32m   4473\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[32m   4474\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4475\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4476\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4477\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4479\u001b[39m total_batch_size = \u001b[38;5;28mself\u001b[39m.args.eval_batch_size * \u001b[38;5;28mself\u001b[39m.args.world_size\n\u001b[32m   4480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_jit_compilation_time\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output.metrics:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/YYK_conda/lib/python3.11/site-packages/transformers/trainer.py:4685\u001b[39m, in \u001b[36mTrainer.evaluation_loop\u001b[39m\u001b[34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4682\u001b[39m         all_inputs.add(inputs_decode)\n\u001b[32m   4683\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4684\u001b[39m     \u001b[38;5;66;03m# Pad labels here, preparing for preprocess_logits_for_metrics in next logits block.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4685\u001b[39m     labels = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_across_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   4686\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4687\u001b[39m     logits = \u001b[38;5;28mself\u001b[39m.accelerator.pad_across_processes(logits, dim=\u001b[32m1\u001b[39m, pad_index=-\u001b[32m100\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/YYK_conda/lib/python3.11/site-packages/accelerate/accelerator.py:3094\u001b[39m, in \u001b[36mAccelerator.pad_across_processes\u001b[39m\u001b[34m(self, tensor, dim, pad_index, pad_first)\u001b[39m\n\u001b[32m   3061\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpad_across_processes\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor, dim=\u001b[32m0\u001b[39m, pad_index=\u001b[32m0\u001b[39m, pad_first=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   3062\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3063\u001b[39m \u001b[33;03m    Recursively pad the tensors in a nested list/tuple/dictionary of tensors from all devices to the same size so\u001b[39;00m\n\u001b[32m   3064\u001b[39m \u001b[33;03m    they can safely be gathered.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3092\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m   3093\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3094\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpad_across_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_first\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_first\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/YYK_conda/lib/python3.11/site-packages/accelerate/utils/operations.py:407\u001b[39m, in \u001b[36mchained_operation.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    406\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    408\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m DistributedOperationException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    409\u001b[39m         operation = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction.\u001b[34m__module__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/YYK_conda/lib/python3.11/site-packages/accelerate/utils/operations.py:677\u001b[39m, in \u001b[36mpad_across_processes\u001b[39m\u001b[34m(tensor, dim, pad_index, pad_first)\u001b[39m\n\u001b[32m    674\u001b[39m     new_tensor[indices] = tensor\n\u001b[32m    675\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m new_tensor\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrecursively_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_pad_across_processes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_on_other_type\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_first\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_first\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/YYK_conda/lib/python3.11/site-packages/accelerate/utils/operations.py:126\u001b[39m, in \u001b[36mrecursively_apply\u001b[39m\u001b[34m(func, data, test_type, error_on_other_type, *args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)(\n\u001b[32m    118\u001b[39m         {\n\u001b[32m    119\u001b[39m             k: recursively_apply(\n\u001b[32m   (...)\u001b[39m\u001b[32m    123\u001b[39m         }\n\u001b[32m    124\u001b[39m     )\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m test_type(data):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m error_on_other_type:\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    129\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported types (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) passed to `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`. Only nested list/tuple/dicts of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    130\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mobjects that are valid for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_type.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` should be passed.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    131\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/YYK_conda/lib/python3.11/site-packages/accelerate/utils/operations.py:657\u001b[39m, in \u001b[36mpad_across_processes.<locals>._pad_across_processes\u001b[39m\u001b[34m(tensor, dim, pad_index, pad_first)\u001b[39m\n\u001b[32m    654\u001b[39m     dim += \u001b[38;5;28mlen\u001b[39m(tensor.shape)\n\u001b[32m    656\u001b[39m \u001b[38;5;66;03m# Gather all sizes\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m size = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    658\u001b[39m sizes = gather(size).cpu()\n\u001b[32m    659\u001b[39m \u001b[38;5;66;03m# Then pad to the maximum size\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ============ 7. è®­ç»ƒ ============\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee5cf07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== ä½¿ç”¨éªŒè¯é›†F1æœ€ä½³æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼° =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ykyang/miniconda3/envs/YYK_conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_loss': 0.3726713955402374, 'test_accuracy': 0.7055630936227951, 'test_f1_macro': 0.6495265230673836, 'test_runtime': 17.4607, 'test_samples_per_second': 84.418, 'test_steps_per_second': 5.326, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# ============ 8. ä½¿ç”¨æœ€ä¼˜æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼° ============\n",
    "print(\"\\n===== ä½¿ç”¨éªŒè¯é›†F1æœ€ä½³æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼° =====\")\n",
    "test_results = trainer.evaluate(eval_dataset=tokenized[\"test\"], metric_key_prefix=\"test\")\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8b1a856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŠ è½½æ¨¡å‹è‡ª: ../checkpoints/flan-t5-sft.json/checkpoint-5000\n",
      "\n",
      "===== å¼€å§‹æµ‹è¯•é›†æ¨ç† =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 185/185 [00:12<00:00, 15.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== æ•´ä½“è¯„ä¼°ç»“æœ =====\n",
      "Accuracy: 0.7171\n",
      "Macro-F1: 0.6616\n",
      "\n",
      "===== åˆ†ç±»è¯¦ç»†æŠ¥å‘Š (PDTB3 å…³ç³»ç±»åˆ«) =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A     0.5636    0.6039    0.5831       154\n",
      "           B     0.7534    0.7278    0.7404       529\n",
      "           C     0.7376    0.7869    0.7615       643\n",
      "           D     0.6518    0.4932    0.5615       148\n",
      "\n",
      "    accuracy                         0.7171      1474\n",
      "   macro avg     0.6766    0.6530    0.6616      1474\n",
      "weighted avg     0.7165    0.7171    0.7152      1474\n",
      "\n",
      "\n",
      "æŠ¥å‘Šå·²ä¿å­˜åˆ°: ../checkpoints/flan-t5-sft.json/checkpoint-5000/test_report.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,5\"\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============ é…ç½® ============\n",
    "model_path = \"../checkpoints/flan-t5-sft.json/checkpoint-5000\"  # âœ… æ”¹æˆä½ çš„checkpointè·¯å¾„\n",
    "test_file = \"/data/ykyang/datasets/pdtb3_T5/pdtb3_test_instruction.json\"\n",
    "\n",
    "max_input_length = 256\n",
    "max_output_length = 8\n",
    "batch_size = 8\n",
    "\n",
    "# ============ åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ ============\n",
    "print(f\"åŠ è½½æ¨¡å‹è‡ª: {model_path}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "# ============ åŠ è½½æµ‹è¯•æ•°æ® ============\n",
    "dataset = load_dataset(\"json\", data_files={\"test\": test_file})[\"test\"]\n",
    "\n",
    "def preprocess(examples):\n",
    "    inputs = [f\"{ins}\\n\\n{inp}\" for ins, inp in zip(examples[\"instruction\"], examples[\"input\"])]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=max_input_length, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "    labels = [t.strip() for t in examples[\"output\"]]\n",
    "    model_inputs[\"labels_text\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "test_data = dataset.map(preprocess, batched=True)\n",
    "\n",
    "# ğŸ”§ è½¬ä¸º torch tensor æ ¼å¼\n",
    "test_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "# ============ æ¨ç† ============\n",
    "loader = DataLoader(test_data, batch_size=batch_size)\n",
    "preds, labels = [], []\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"\\n===== å¼€å§‹æµ‹è¯•é›†æ¨ç† =====\")\n",
    "for batch in tqdm(loader):\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_output_length,\n",
    "        )\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    preds.extend([p.strip() for p in decoded_preds])\n",
    "\n",
    "# âœ… ä»åŸå§‹ Dataset æå– labels\n",
    "labels = [l.strip() for l in test_data[\"labels_text\"]]\n",
    "\n",
    "# ============ è®¡ç®—æ€»ä½“æŒ‡æ ‡ ============\n",
    "acc = accuracy_score(labels, preds)\n",
    "f1_macro = f1_score(labels, preds, average=\"macro\")\n",
    "\n",
    "print(\"\\n===== æ•´ä½“è¯„ä¼°ç»“æœ =====\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Macro-F1: {f1_macro:.4f}\")\n",
    "\n",
    "# ============ å„ç±»åˆ«è¯¦ç»†æŠ¥å‘Š ============\n",
    "print(\"\\n===== åˆ†ç±»è¯¦ç»†æŠ¥å‘Š (PDTB3 å…³ç³»ç±»åˆ«) =====\")\n",
    "print(classification_report(labels, preds, digits=4))\n",
    "\n",
    "# ============ ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶ ============\n",
    "save_path = os.path.join(model_path, \"test_report.txt\")\n",
    "with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"===== Overall Metrics =====\\n\")\n",
    "    f.write(f\"Accuracy: {acc:.4f}\\n\")\n",
    "    f.write(f\"Macro-F1: {f1_macro:.4f}\\n\\n\")\n",
    "    f.write(\"===== Per-class Report =====\\n\")\n",
    "    f.write(classification_report(labels, preds, digits=4))\n",
    "print(f\"\\næŠ¥å‘Šå·²ä¿å­˜åˆ°: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6f2cda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŠ è½½æ¨¡å‹è‡ª: ../checkpoints/flan-t5-sft.json/checkpoint-5350\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "980d9b0902a641d3a1172258a02c959f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1474 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== å¼€å§‹æµ‹è¯•é›†æ¨ç† =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 185/185 [00:12<00:00, 15.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== æ•´ä½“è¯„ä¼°ç»“æœ =====\n",
      "Accuracy: 0.7185\n",
      "Macro-F1: 0.6666\n",
      "\n",
      "===== åˆ†ç±»è¯¦ç»†æŠ¥å‘Š (PDTB3 å…³ç³»ç±»åˆ«) =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A     0.5575    0.6299    0.5915       154\n",
      "           B     0.7629    0.7240    0.7430       529\n",
      "           C     0.7368    0.7838    0.7596       643\n",
      "           D     0.6579    0.5068    0.5725       148\n",
      "\n",
      "    accuracy                         0.7185      1474\n",
      "   macro avg     0.6788    0.6611    0.6666      1474\n",
      "weighted avg     0.7195    0.7185    0.7173      1474\n",
      "\n",
      "\n",
      "æŠ¥å‘Šå·²ä¿å­˜åˆ°: ../checkpoints/flan-t5-sft.json/checkpoint-5350/test_report.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============ é…ç½® ============\n",
    "model_path = \"../checkpoints/flan-t5-sft.json/checkpoint-5350\"  # âœ… æ”¹æˆä½ çš„checkpointè·¯å¾„\n",
    "test_file = \"/data/ykyang/datasets/pdtb3_T5/pdtb3_test_instruction.json\"\n",
    "\n",
    "max_input_length = 256\n",
    "max_output_length = 8\n",
    "batch_size = 8\n",
    "\n",
    "# ============ åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ ============\n",
    "print(f\"åŠ è½½æ¨¡å‹è‡ª: {model_path}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "# ============ åŠ è½½æµ‹è¯•æ•°æ® ============\n",
    "dataset = load_dataset(\"json\", data_files={\"test\": test_file})[\"test\"]\n",
    "\n",
    "def preprocess(examples):\n",
    "    inputs = [f\"{ins}\\n\\n{inp}\" for ins, inp in zip(examples[\"instruction\"], examples[\"input\"])]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=max_input_length, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "    labels = [t.strip() for t in examples[\"output\"]]\n",
    "    model_inputs[\"labels_text\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "test_data = dataset.map(preprocess, batched=True)\n",
    "\n",
    "# ğŸ”§ è½¬ä¸º torch tensor æ ¼å¼\n",
    "test_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "# ============ æ¨ç† ============\n",
    "loader = DataLoader(test_data, batch_size=batch_size)\n",
    "preds, labels = [], []\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"\\n===== å¼€å§‹æµ‹è¯•é›†æ¨ç† =====\")\n",
    "for batch in tqdm(loader):\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_output_length,\n",
    "        )\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    preds.extend([p.strip() for p in decoded_preds])\n",
    "\n",
    "# âœ… ä»åŸå§‹ Dataset æå– labels\n",
    "labels = [l.strip() for l in test_data[\"labels_text\"]]\n",
    "\n",
    "# ============ è®¡ç®—æ€»ä½“æŒ‡æ ‡ ============\n",
    "acc = accuracy_score(labels, preds)\n",
    "f1_macro = f1_score(labels, preds, average=\"macro\")\n",
    "\n",
    "print(\"\\n===== æ•´ä½“è¯„ä¼°ç»“æœ =====\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Macro-F1: {f1_macro:.4f}\")\n",
    "\n",
    "# ============ å„ç±»åˆ«è¯¦ç»†æŠ¥å‘Š ============\n",
    "print(\"\\n===== åˆ†ç±»è¯¦ç»†æŠ¥å‘Š (PDTB3 å…³ç³»ç±»åˆ«) =====\")\n",
    "print(classification_report(labels, preds, digits=4))\n",
    "\n",
    "# ============ ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶ ============\n",
    "save_path = os.path.join(model_path, \"test_report.txt\")\n",
    "with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"===== Overall Metrics =====\\n\")\n",
    "    f.write(f\"Accuracy: {acc:.4f}\\n\")\n",
    "    f.write(f\"Macro-F1: {f1_macro:.4f}\\n\\n\")\n",
    "    f.write(\"===== Per-class Report =====\\n\")\n",
    "    f.write(classification_report(labels, preds, digits=4))\n",
    "print(f\"\\næŠ¥å‘Šå·²ä¿å­˜åˆ°: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a5367a",
   "metadata": {},
   "source": [
    "æ— æ•°æ®å¢å¼ºçš„sft\n",
    "===== æ•´ä½“è¯„ä¼°ç»“æœ =====\n",
    "Accuracy: 0.7144\n",
    "Macro-F1: 0.6592\n",
    "===== åˆ†ç±»è¯¦ç»†æŠ¥å‘Š (PDTB3 å…³ç³»ç±»åˆ«) =====\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           A     0.5285    0.6623    0.5879       154\n",
    "           B     0.7505    0.7448    0.7476       529\n",
    "           C     0.7485    0.7589    0.7537       643\n",
    "           D     0.6635    0.4662    0.5476       148\n",
    "\n",
    "    accuracy                         0.7144      1474\n",
    "   macro avg     0.6727    0.6581    0.6592      1474\n",
    "weighted avg     0.7177    0.7144    0.7135      1474\n",
    "\n",
    "å°†expansionæ‰©å±•ä¸ºtemporalä¸€ç™¾æ¡+å‡ æ¡å…¶ä»–\n",
    "===== æ•´ä½“è¯„ä¼°ç»“æœ =====\n",
    "Accuracy: 0.6981\n",
    "Macro-F1: 0.6413\n",
    "\n",
    "===== åˆ†ç±»è¯¦ç»†æŠ¥å‘Š (PDTB3 å…³ç³»ç±»åˆ«) =====\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           A     0.5208    0.6494    0.5780       154\n",
    "           B     0.7480    0.7127    0.7299       529\n",
    "           C     0.7243    0.7558    0.7397       643\n",
    "           D     0.6168    0.4459    0.5176       148\n",
    "\n",
    "    accuracy                         0.6981      1474\n",
    "   macro avg     0.6525    0.6409    0.6413      1474\n",
    "weighted avg     0.7008    0.6981    0.6970      1474\n",
    "\n",
    "å°†expansionæ‰©å±•ä¸ºtemporalä¸€ç™¾æ¡ å¹¶ä¸”åˆ é™¤åŸæœ¬çš„expansion\n",
    "===== æ•´ä½“è¯„ä¼°ç»“æœ =====\n",
    "Accuracy: 0.7069\n",
    "Macro-F1: 0.6556\n",
    "\n",
    "===== åˆ†ç±»è¯¦ç»†æŠ¥å‘Š (PDTB3 å…³ç³»ç±»åˆ«) =====\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           A     0.5625    0.6429    0.6000       154\n",
    "           B     0.7565    0.7108    0.7329       529\n",
    "           C     0.7182    0.7729    0.7446       643\n",
    "           D     0.6422    0.4730    0.5447       148\n",
    "\n",
    "    accuracy                         0.7069      1474\n",
    "   macro avg     0.6699    0.6499    0.6556      1474\n",
    "weighted avg     0.7081    0.7069    0.7052      1474\n",
    "\n",
    "æ–¹æ³•äºŒæ‰©å±•expansionçš„86æ¡æ•°æ®ä¸ºtemporal\n",
    "===== æ•´ä½“è¯„ä¼°ç»“æœ =====\n",
    "Accuracy: 0.7096\n",
    "Macro-F1: 0.6528\n",
    "\n",
    "===== åˆ†ç±»è¯¦ç»†æŠ¥å‘Š (PDTB3 å…³ç³»ç±»åˆ«) =====\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           A     0.6308    0.5325    0.5775       154\n",
    "           B     0.7495    0.7240    0.7365       529\n",
    "           C     0.7157    0.7869    0.7496       643\n",
    "           D     0.5952    0.5068    0.5474       148\n",
    "\n",
    "    accuracy                         0.7096      1474\n",
    "   macro avg     0.6728    0.6375    0.6528      1474\n",
    "weighted avg     0.7069    0.7096    0.7066      1474\n",
    "\n",
    "æ–¹æ³•äºŒæ‰©å±•expansionçš„86æ¡æ•°æ®ä¸ºtemporalä¸”åˆ é™¤åŸå§‹æ ·æœ¬\n",
    "===== æ•´ä½“è¯„ä¼°ç»“æœ =====\n",
    "Accuracy: 0.7049\n",
    "Macro-F1: 0.6544\n",
    "\n",
    "===== åˆ†ç±»è¯¦ç»†æŠ¥å‘Š (PDTB3 å…³ç³»ç±»åˆ«) =====\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           A     0.5808    0.6299    0.6044       154\n",
    "           B     0.7578    0.6919    0.7233       529\n",
    "           C     0.7103    0.7854    0.7459       643\n",
    "           D     0.6283    0.4797    0.5441       148\n",
    "\n",
    "    accuracy                         0.7049      1474\n",
    "   macro avg     0.6693    0.6467    0.6544      1474\n",
    "weighted avg     0.7056    0.7049    0.7028      1474\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YYK_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
