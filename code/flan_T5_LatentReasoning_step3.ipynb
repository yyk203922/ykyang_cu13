{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "909e010c",
   "metadata": {},
   "source": [
    "#### Âä®ÊÄÅÂêØÁî®‰∏≠Èó¥ÁõëÁù£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "42892222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_latent_idrr_corrected.py\n",
    "import os, json, random\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup,T5EncoderModel\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "SEED = 42\n",
    "EPOCHS = 4\n",
    "BATCH_SIZE = 8\n",
    "LR =3e-5\n",
    "MAX_LEN = 256\n",
    "MODEL_NAME = \"../models/flan-t5-base\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DATA_DIR = \"../datasets/pdtb3_T5\"\n",
    "OUT_DIR = \"outputs_latent_idrr_denoise_Intersupervised\"\n",
    "LOOP_K = 3\n",
    "AUX_WEIGHT = 0.05\n",
    "NOISE_STD = 0.02\n",
    "LAMBDA_DENOISE = 0\n",
    "DENOISE_LAST_ONLY = True\n",
    "AUX_WARMUP_EPOCHS = 1  # üöÄ Âä®ÊÄÅÂêØÁî®‰∏≠Èó¥ÁõëÁù£\n",
    "NUM_WORKERS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f25bbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Utilities\n",
    "# -----------------------\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed()\n",
    "\n",
    "# -----------------------\n",
    "# Dataset\n",
    "# -----------------------\n",
    "class IDRRDataset(Dataset):\n",
    "    def __init__(self, path, tokenizer, max_len=MAX_LEN, prompt_prefix=\"relation classification: Arg1: \"):\n",
    "        self.samples = []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    self.samples.append(json.loads(line))\n",
    "        self.tok = tokenizer; self.max_len = max_len; self.prompt = prompt_prefix\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.samples[idx]\n",
    "        text = f\"{self.prompt}{s['Arg1']}  Arg2: {s['Arg2']}\"\n",
    "        inputs = self.tok(text, truncation=True, padding='max_length', max_length=self.max_len, return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": s[\"Label\"]\n",
    "        }\n",
    "\n",
    "def collate_fn(batch, tokenizer):\n",
    "    return {\n",
    "        \"input_ids\": torch.stack([b['input_ids'] for b in batch]),\n",
    "        \"attention_mask\": torch.stack([b['attention_mask'] for b in batch]),\n",
    "        \"labels\": [b['label'] for b in batch]\n",
    "    }\n",
    "\n",
    "def build_label_maps(paths: List[str]):\n",
    "    labels = set()\n",
    "    for p in paths:\n",
    "        with open(p, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    labels.add(json.loads(line)['Label'])\n",
    "    labels = sorted(list(labels))\n",
    "    id2label = {i:l for i,l in enumerate(labels)}\n",
    "    label2id = {l:i for i,l in id2label.items()}\n",
    "    return label2id, id2label\n",
    "\n",
    "# -----------------------\n",
    "# Model components\n",
    "# -----------------------\n",
    "class RefinerBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead=8, dim_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, batch_first=True)\n",
    "        self.ff = nn.Sequential(nn.Linear(d_model, dim_ff), nn.GELU(), nn.Linear(dim_ff, d_model))\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, r, H, H_mask=None):\n",
    "        key_padding_mask = (H_mask == 0) if H_mask is not None else None\n",
    "        att_out, _ = self.attn(query=r, key=H, value=H, key_padding_mask=key_padding_mask)\n",
    "        r = self.ln1(r + self.dropout(att_out))\n",
    "        r = self.ln2(r + self.dropout(self.ff(r)))\n",
    "        return r\n",
    "\n",
    "class LatentIDRRModel(nn.Module):\n",
    "    def __init__(self, model_name=MODEL_NAME, label2id=None, loop_k=LOOP_K, aux_weight=AUX_WEIGHT, noise_std=NOISE_STD, use_separate_refiners=False):\n",
    "        super().__init__()\n",
    "        assert label2id is not None\n",
    "        base = AutoModel.from_pretrained(model_name)\n",
    "        # if seq2seq (T5), get encoder\n",
    "        self.encoder = base.get_encoder() if hasattr(base, \"get_encoder\") else base\n",
    "        self.loop_k = loop_k\n",
    "        self.noise_std = noise_std\n",
    "        self.aux_weight = aux_weight\n",
    "        self.d_model = self.encoder.config.hidden_size\n",
    "        self.use_separate_refiners = use_separate_refiners\n",
    "        if use_separate_refiners:\n",
    "            self.refiners = nn.ModuleList([RefinerBlock(d_model=self.d_model) for _ in range(self.loop_k)])\n",
    "        else:\n",
    "            self.refiner = RefinerBlock(d_model=self.d_model)\n",
    "        self.pool_proj = nn.Linear(self.d_model, self.d_model)\n",
    "        # classifier\n",
    "        hid = max(self.d_model//2, 32)\n",
    "        self.classifier = nn.Sequential(nn.Linear(self.d_model, hid), nn.GELU(), nn.LayerNorm(hid), nn.Linear(hid, len(label2id)))\n",
    "        self.label2id = label2id\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # ---- 1Ô∏è‚É£ ÁºñÁ†ÅËæìÂÖ• ----\n",
    "        enc = self.encoder(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        H = enc.last_hidden_state   # (B,L,D) ÂéüÂßã‰∏ä‰∏ãÊñáÈöêÁ©∫Èó¥Ë°®ÂæÅ (contextual embeddings)\n",
    "        mask = attention_mask.unsqueeze(-1)  # (B,L,1)\n",
    "\n",
    "        # ---- 2Ô∏è‚É£ Âπ≥ÂùáÊ±†ÂåñÂæóÂà∞ÂàùÂßãÂÖ≥Á≥ªË°®Á§∫ r ----\n",
    "        # r Áõ∏ÂΩì‰∫é ‚ÄúÂàùÂßãÊΩúÂú®ÊÄùÁª¥Áä∂ÊÄÅÔºàinitial latent reasoning stateÔºâ‚Äù\n",
    "        sum_h = (H * mask).sum(dim=1)\n",
    "        lengths = mask.sum(dim=1).clamp(min=1)\n",
    "        r = (sum_h / lengths).unsqueeze(1)   # (B,1,D)\n",
    "        r = self.pool_proj(r)\n",
    "\n",
    "        aux_logits = []\n",
    "\n",
    "        # =====================================================\n",
    "        # üöÄ 3Ô∏è‚É£ ËøõÂÖ•‚ÄúÂæ™ÁéØÈöêÂºèÊÄùËÄÉ (Iterative Latent Reasoning Loop)‚Äù\n",
    "        # =====================================================\n",
    "        for t in range(self.loop_k):\n",
    "\n",
    "            # ---- (a) Âô™Â£∞Ê≥®ÂÖ• (Latent Noise Injection) ----\n",
    "            # Âú®ÊØè‰∏ÄËΩÆÂæ™ÁéØÂâçÔºåÂØπ‰∏ä‰∏ãÊñáË°®ÂæÅ H Âä†ÈöèÊú∫Âô™Â£∞Ôºå\n",
    "            # ÁõÆÁöÑÊòØÊ®°Êãü‚Äú‰ø°ÊÅØ‰∏çÁ°ÆÂÆöÊÄß‚ÄùÂπ∂Ëø´‰ΩøÊ®°ÂûãÂ≠¶‰ºö‰ªéÂô™Â£∞‰∏≠ÊÅ¢Â§çËØ≠‰πâ„ÄÇ\n",
    "            # Ëøô‰∏ÄÊ≠•ÊòØ‚ÄúÂéªÂô™Êé®ÁêÜ‚ÄùÁöÑËæìÂÖ•ÂáÜÂ§á„ÄÇ\n",
    "            if self.noise_std is not None and self.noise_std > 0:\n",
    "                noisy_H = H + torch.randn_like(H) * (self.noise_std)\n",
    "            else:\n",
    "                noisy_H = H\n",
    "\n",
    "            # ---- (b) Á≤æÂåñÊõ¥Êñ∞ (Refinement Step) ----\n",
    "            # RefinerBlock ‰ΩøÁî®Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåËÆ©ÂΩìÂâçÁöÑÂÖ≥Á≥ªÁä∂ÊÄÅ r\n",
    "            # Âú® noisy_H ÁöÑ‰∏ä‰∏ãÊñá‰∏ãÂæóÂà∞Êõ¥Êñ∞„ÄÇ\n",
    "            # ÂèØÁêÜËß£‰∏∫ ‚Äúr Âú®Âô™Â£∞Âπ≤Êâ∞ÁöÑÊΩúÁ©∫Èó¥‰∏≠ÈáçÊñ∞ÊÄùËÄÉÂπ∂Ê†°Ê≠£Ëá™Ë∫´ËØ≠‰πâ‚Äù„ÄÇ\n",
    "            if self.use_separate_refiners:\n",
    "                r = r + self.refiners[t](r, noisy_H, H_mask=attention_mask)\n",
    "            else:\n",
    "                r = r + self.refiner(r, noisy_H, H_mask=attention_mask)\n",
    "\n",
    "            # ---- (c) ‰∏≠Èó¥ÁõëÁù£ (Auxiliary Supervision) ----\n",
    "            # Â¶ÇÊûúÂêØÁî®ËæÖÂä©ÁõëÁù£ÔºåÊØè‰∏ÄËΩÆÂæ™ÁéØÈÉΩ‰ºö‰∫ßÁîü‰∏Ä‰∏™È¢ÑÊµãÔºå\n",
    "            # Áî®‰∫éÈºìÂä±Ê®°ÂûãÂú®ÊØèÊ¨°‚ÄúÊÄùËÄÉ‚ÄùÂêéÈÉΩÊúâÊõ¥Ê∏ÖÊô∞ÁöÑÂÖ≥Á≥ªË°®Á§∫„ÄÇ\n",
    "            if self.aux_weight is not None and self.aux_weight > 0:\n",
    "                aux_logits.append(self.classifier(r.squeeze(1)))\n",
    "\n",
    "        # =====================================================\n",
    "        # üß© 4Ô∏è‚É£ ÊúÄÁªàÂàÜÁ±ªÔºöËæìÂá∫ÊúÄÂêé‰∏ÄÊ≠•Á≤æÂåñÁªìÊûú\n",
    "        # =====================================================\n",
    "        logits = self.classifier(r.squeeze(1))\n",
    "        return logits, aux_logits\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Train / Eval\n",
    "# -----------------------\n",
    "def encode_labels(labels, label2id):\n",
    "    return torch.tensor([label2id[l] for l in labels], dtype=torch.long)\n",
    "\n",
    "def evaluate(model, dataloader, label2id, device):\n",
    "    model.eval()\n",
    "    id2label = {v:k for k,v in label2id.items()}\n",
    "    preds, gts = [], []\n",
    "    with torch.no_grad():\n",
    "        for b in tqdm(dataloader, desc=\"Eval\", leave=False):\n",
    "            input_ids = b['input_ids'].to(device)\n",
    "            attention_mask = b['attention_mask'].to(device)\n",
    "            labels_raw = b['labels']\n",
    "            logits, _ = model(input_ids, attention_mask)\n",
    "            pred_ids = logits.argmax(dim=-1).cpu().numpy().tolist()\n",
    "            preds.extend([id2label[i] for i in pred_ids])\n",
    "            gts.extend(labels_raw)\n",
    "    macro = f1_score(gts, preds, average='macro', labels=list(label2id.keys()))\n",
    "    acc = accuracy_score(gts, preds)\n",
    "    print(classification_report(gts, preds, digits=4))\n",
    "    return {\"macro_f1\": macro, \"accuracy\": acc}\n",
    "\n",
    "def train(model, train_dl, dev_dl, optimizer, scheduler, label2id, device):\n",
    "    model.to(device)\n",
    "    best_f1 = 0.0\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        # linear warm-up of aux: increases from 0 -> model.aux_weight over AUX_WARMUP_EPOCHS\n",
    "        if AUX_WARMUP_EPOCHS <= 0:\n",
    "            current_aux_w = float(model.aux_weight or 0.0)\n",
    "        else:\n",
    "            current_aux_w = float(model.aux_weight or 0.0) * min(1.0, (epoch + 1) / float(AUX_WARMUP_EPOCHS))\n",
    "        use_aux = current_aux_w > 0.0\n",
    "\n",
    "        pbar = tqdm(train_dl, desc=f\"Train Epoch {epoch+1} (aux_w={current_aux_w:.4f})\")\n",
    "        for b in pbar:\n",
    "            input_ids = b['input_ids'].to(device)\n",
    "            attention_mask = b['attention_mask'].to(device)\n",
    "            labels_raw = b['labels']\n",
    "            labels = encode_labels(labels_raw, label2id).to(device)\n",
    "\n",
    "            logits, aux_logits = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            if use_aux and len(aux_logits) > 0:\n",
    "                for al in aux_logits:\n",
    "                    loss = loss + current_aux_w * loss_fn(al, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            pbar.set_postfix({\"loss\": f\"{np.mean(losses[-50:]):.4f}\"})\n",
    "\n",
    "        dev_metrics = evaluate(model, dev_dl, label2id, device)\n",
    "        print(f\"Epoch {epoch+1} dev macro-F1: {dev_metrics['macro_f1']:.4f}, acc: {dev_metrics['accuracy']:.4f}\")\n",
    "        if dev_metrics['macro_f1'] > best_f1:\n",
    "            best_f1 = dev_metrics['macro_f1']\n",
    "            os.makedirs(OUT_DIR, exist_ok=True)\n",
    "            torch.save(model.state_dict(), os.path.join(OUT_DIR, \"best_model.pt\"))\n",
    "            print(\"‚úÖ Saved best model.\")\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "# -----------------------\n",
    "# Main\n",
    "# -----------------------\n",
    "def main():\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    label2id, id2label = build_label_maps([\n",
    "        os.path.join(DATA_DIR, \"train.jsonl\"),\n",
    "        os.path.join(DATA_DIR, \"dev.jsonl\"),\n",
    "        os.path.join(DATA_DIR, \"test.jsonl\")\n",
    "    ])\n",
    "    print(\"Labels:\", label2id)\n",
    "\n",
    "    train_ds = IDRRDataset(os.path.join(DATA_DIR, \"train.jsonl\"), tokenizer)\n",
    "    dev_ds = IDRRDataset(os.path.join(DATA_DIR, \"dev.jsonl\"), tokenizer)\n",
    "    test_ds = IDRRDataset(os.path.join(DATA_DIR, \"test.jsonl\"), tokenizer)\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          collate_fn=lambda x: collate_fn(x, tokenizer), num_workers=NUM_WORKERS)\n",
    "    dev_dl = DataLoader(dev_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        collate_fn=lambda x: collate_fn(x, tokenizer), num_workers=NUM_WORKERS)\n",
    "    test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                         collate_fn=lambda x: collate_fn(x, tokenizer), num_workers=NUM_WORKERS)\n",
    "\n",
    "    model = LatentIDRRModel(\n",
    "        model_name=MODEL_NAME,\n",
    "        label2id=label2id,\n",
    "        loop_k=LOOP_K,\n",
    "        aux_weight=AUX_WEIGHT,\n",
    "        noise_std=NOISE_STD,\n",
    "        use_separate_refiners=False\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    total_steps = len(train_dl) * EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=max(1,int(0.06*total_steps)), num_training_steps=max(1,total_steps))\n",
    "\n",
    "    print(\"Device:\", DEVICE)\n",
    "    train(model, train_dl, dev_dl, optimizer, scheduler, label2id, DEVICE)\n",
    "\n",
    "    # final test\n",
    "    best_path = os.path.join(OUT_DIR, \"best_model.pt\")\n",
    "    if os.path.exists(best_path):\n",
    "        model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n",
    "        print(\"\\nüß™ Final Test Evaluation\")\n",
    "        test_metrics = evaluate(model, test_dl, label2id, DEVICE)\n",
    "        print(f\"Final Test Macro-F1: {test_metrics['macro_f1']:.4f}, Acc: {test_metrics['accuracy']:.4f}\")\n",
    "    else:\n",
    "        print(\"No best model saved, skipping test eval.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2effef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- \n",
    "# Utilities\n",
    "# -----------------------\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed()\n",
    "\n",
    "# -----------------------\n",
    "# Dataset\n",
    "# -----------------------\n",
    "class IDRRDataset(Dataset):\n",
    "    def __init__(self, path, tokenizer, max_len=MAX_LEN, prompt_prefix=\"relation classification: Arg1: \"):\n",
    "        self.samples = []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    self.samples.append(json.loads(line))\n",
    "        self.tok = tokenizer; self.max_len = max_len; self.prompt = prompt_prefix\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.samples[idx]\n",
    "        text = f\"{self.prompt}{s['Arg1']}  Arg2: {s['Arg2']}\"\n",
    "        inputs = self.tok(text, truncation=True, padding='max_length', max_length=self.max_len, return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": s[\"Label\"]\n",
    "        }\n",
    "\n",
    "def collate_fn(batch, tokenizer):\n",
    "    return {\n",
    "        \"input_ids\": torch.stack([b['input_ids'] for b in batch]),\n",
    "        \"attention_mask\": torch.stack([b['attention_mask'] for b in batch]),\n",
    "        \"labels\": [b['label'] for b in batch]\n",
    "    }\n",
    "\n",
    "def build_label_maps(paths: List[str]):\n",
    "    labels = set()\n",
    "    for p in paths:\n",
    "        with open(p, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    labels.add(json.loads(line)['Label'])\n",
    "    labels = sorted(list(labels))\n",
    "    id2label = {i:l for i,l in enumerate(labels)}\n",
    "    label2id = {l:i for i,l in id2label.items()}\n",
    "    return label2id, id2label\n",
    "\n",
    "# -----------------------\n",
    "# Model components\n",
    "# -----------------------\n",
    "class RefinerBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead=8, dim_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, batch_first=True)\n",
    "        self.ff = nn.Sequential(nn.Linear(d_model, dim_ff), nn.GELU(), nn.Linear(dim_ff, d_model))\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, r, H, H_mask=None):\n",
    "        key_padding_mask = (H_mask == 0) if H_mask is not None else None\n",
    "        att_out, _ = self.attn(query=r, key=H, value=H, key_padding_mask=key_padding_mask)\n",
    "        r = self.ln1(r + self.dropout(att_out))\n",
    "        r = self.ln2(r + self.dropout(self.ff(r)))\n",
    "        return r\n",
    "\n",
    "\n",
    "class LatentIDRRModel(nn.Module):\n",
    "    def __init__(self, model_name=\"google/flan-t5-base\", num_labels=11,\n",
    "                 loop_K=2, noise_std=0.02,\n",
    "                 aux_weight=0.05, lambda_denoise=0.01,\n",
    "                 denoise_last_only=True, aux_warmup_epochs=1,\n",
    "                 use_separate_refiners=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = T5EncoderModel.from_pretrained(model_name)\n",
    "        self.loop_K = loop_K\n",
    "        self.noise_std = noise_std\n",
    "        self.aux_weight = aux_weight\n",
    "        self.lambda_denoise = lambda_denoise\n",
    "        self.denoise_last_only = denoise_last_only\n",
    "        self.aux_warmup_epochs = aux_warmup_epochs\n",
    "        self.use_separate_refiners = use_separate_refiners\n",
    "\n",
    "        hidden_size = self.encoder.config.d_model\n",
    "        self.pool_proj = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        def make_refiner():\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(hidden_size * 2, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size, hidden_size)\n",
    "            )\n",
    "\n",
    "        if self.use_separate_refiners:\n",
    "            self.refiners = nn.ModuleList([make_refiner() for _ in range(loop_K)])\n",
    "        else:\n",
    "            self.refiner = make_refiner()\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "        self.aux_head = nn.Linear(hidden_size, num_labels)\n",
    "\n",
    "    def refine_once(self, r, H, H_mask, refiner):\n",
    "        B, L, D = H.size()\n",
    "        r_expand = r.expand(B, L, D)\n",
    "        concat = torch.cat([r_expand, H], dim=-1)\n",
    "        delta = refiner(concat)\n",
    "        return (delta * H_mask.unsqueeze(-1)).mean(dim=1, keepdim=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None, epoch=0):\n",
    "        enc = self.encoder(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        H = enc.last_hidden_state  # (B, L, D)\n",
    "\n",
    "        mask = attention_mask.unsqueeze(-1)\n",
    "        sum_h = (H * mask).sum(dim=1)\n",
    "        lengths = mask.sum(dim=1).clamp(min=1)\n",
    "        r_clean = (sum_h / lengths).unsqueeze(1)\n",
    "        r_clean_proj = self.pool_proj(r_clean)\n",
    "\n",
    "        if self.noise_std > 0:\n",
    "            H_noisy = H + torch.randn_like(H) * self.noise_std\n",
    "        else:\n",
    "            H_noisy = H\n",
    "\n",
    "        r = self.pool_proj(r_clean)\n",
    "        aux_logits, denoise_preds = [], []\n",
    "\n",
    "        for t in range(self.loop_K):\n",
    "            if self.use_separate_refiners:\n",
    "                r = r + self.refine_once(r, H_noisy, attention_mask, self.refiners[t])\n",
    "            else:\n",
    "                r = r + self.refine_once(r, H_noisy, attention_mask, self.refiner)\n",
    "            denoise_preds.append(r.squeeze(1))\n",
    "\n",
    "            if (self.aux_weight > 0) and (epoch >= self.aux_warmup_epochs):\n",
    "                aux_logits.append(self.aux_head(r.squeeze(1)))\n",
    "\n",
    "        logits = self.classifier(r.squeeze(1))\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_main = F.cross_entropy(logits, labels)\n",
    "\n",
    "            # ‰∏≠Èó¥ÁõëÁù£\n",
    "            if aux_logits:\n",
    "                loss_aux = sum(F.cross_entropy(aux, labels) for aux in aux_logits) / len(aux_logits)\n",
    "            else:\n",
    "                loss_aux = torch.tensor(0.0, device=logits.device)\n",
    "\n",
    "            # ÂéªÂô™Á∫¶ÊùüÊçüÂ§±\n",
    "            if self.lambda_denoise > 0 and len(denoise_preds) > 0:\n",
    "                if self.denoise_last_only:\n",
    "                    loss_denoise = F.mse_loss(denoise_preds[-1], r_clean_proj.squeeze(1))\n",
    "                else:\n",
    "                    loss_denoise = sum(F.mse_loss(r_t, r_clean_proj.squeeze(1)) for r_t in denoise_preds) / len(denoise_preds)\n",
    "            else:\n",
    "                loss_denoise = torch.tensor(0.0, device=logits.device)\n",
    "\n",
    "            loss = loss_main + self.aux_weight * loss_aux + self.lambda_denoise * loss_denoise\n",
    "\n",
    "            return {\n",
    "                \"loss\": loss,\n",
    "                \"loss_main\": loss_main.detach(),\n",
    "                \"loss_aux\": loss_aux.detach(),\n",
    "                \"loss_denoise\": loss_denoise.detach(),\n",
    "                \"logits\": logits\n",
    "            }\n",
    "\n",
    "        return {\"logits\": logits}\n",
    "\n",
    "# -----------------------\n",
    "# Train / Eval\n",
    "# -----------------------\n",
    "def encode_labels(labels, label2id):\n",
    "    return torch.tensor([label2id[l] for l in labels], dtype=torch.long)\n",
    "\n",
    "def evaluate(model, dataloader, label2id, device):\n",
    "    model.eval()\n",
    "    id2label = {v:k for k,v in label2id.items()}\n",
    "    preds, gts = [], []\n",
    "    with torch.no_grad():\n",
    "        for b in tqdm(dataloader, desc=\"Eval\", leave=False):\n",
    "            input_ids = b['input_ids'].to(device)\n",
    "            attention_mask = b['attention_mask'].to(device)\n",
    "            labels_raw = b['labels']\n",
    "            out = model(input_ids, attention_mask)\n",
    "            logits = out[\"logits\"]\n",
    "            pred_ids = logits.argmax(dim=-1).cpu().numpy().tolist()\n",
    "            preds.extend([id2label[i] for i in pred_ids])\n",
    "            gts.extend(labels_raw)\n",
    "    macro = f1_score(gts, preds, average='macro', labels=list(label2id.keys()))\n",
    "    acc = accuracy_score(gts, preds)\n",
    "    print(classification_report(gts, preds, digits=4))\n",
    "    return {\"macro_f1\": macro, \"accuracy\": acc}\n",
    "\n",
    "def train(model, train_dl, dev_dl, optimizer, scheduler, label2id, device):\n",
    "    model.to(device)\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        pbar = tqdm(train_dl, desc=f\"Train Epoch {epoch+1}\")\n",
    "\n",
    "        for b in pbar:\n",
    "            input_ids = b['input_ids'].to(device)\n",
    "            attention_mask = b['attention_mask'].to(device)\n",
    "            labels = encode_labels(b['labels'], label2id).to(device)\n",
    "\n",
    "            out = model(input_ids, attention_mask, labels=labels, epoch=epoch)\n",
    "            loss = out[\"loss\"]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            pbar.set_postfix({\"loss\": f\"{np.mean(losses[-50:]):.4f}\"})\n",
    "\n",
    "        dev_metrics = evaluate(model, dev_dl, label2id, device)\n",
    "        print(f\"Epoch {epoch+1} dev macro-F1: {dev_metrics['macro_f1']:.4f}, acc: {dev_metrics['accuracy']:.4f}\")\n",
    "        if dev_metrics['macro_f1'] > best_f1:\n",
    "            best_f1 = dev_metrics['macro_f1']\n",
    "            os.makedirs(OUT_DIR, exist_ok=True)\n",
    "            torch.save(model.state_dict(), os.path.join(OUT_DIR, \"best_model.pt\"))\n",
    "            print(\"‚úÖ Saved best model.\")\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "# -----------------------\n",
    "# Main\n",
    "# -----------------------\n",
    "def main():\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    label2id, id2label = build_label_maps([\n",
    "        os.path.join(DATA_DIR, \"train.jsonl\"),\n",
    "        os.path.join(DATA_DIR, \"dev.jsonl\"),\n",
    "        os.path.join(DATA_DIR, \"test.jsonl\")\n",
    "    ])\n",
    "    print(\"Labels:\", label2id)\n",
    "\n",
    "    train_ds = IDRRDataset(os.path.join(DATA_DIR, \"train.jsonl\"), tokenizer)\n",
    "    dev_ds = IDRRDataset(os.path.join(DATA_DIR, \"dev.jsonl\"), tokenizer)\n",
    "    test_ds = IDRRDataset(os.path.join(DATA_DIR, \"test.jsonl\"), tokenizer)\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          collate_fn=lambda x: collate_fn(x, tokenizer), num_workers=NUM_WORKERS)\n",
    "    dev_dl = DataLoader(dev_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        collate_fn=lambda x: collate_fn(x, tokenizer), num_workers=NUM_WORKERS)\n",
    "    test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                         collate_fn=lambda x: collate_fn(x, tokenizer), num_workers=NUM_WORKERS)\n",
    "\n",
    "    model = LatentIDRRModel(\n",
    "        model_name=MODEL_NAME,\n",
    "        num_labels=len(label2id),\n",
    "        loop_K=LOOP_K,\n",
    "        aux_weight=AUX_WEIGHT,\n",
    "        noise_std=NOISE_STD,\n",
    "        lambda_denoise=LAMBDA_DENOISE,\n",
    "        denoise_last_only=DENOISE_LAST_ONLY,\n",
    "        aux_warmup_epochs=AUX_WARMUP_EPOCHS,\n",
    "        use_separate_refiners=False\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    total_steps = len(train_dl) * EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=max(1,int(0.06*total_steps)),\n",
    "        num_training_steps=max(1,total_steps)\n",
    "    )\n",
    "\n",
    "    print(\"Device:\", DEVICE)\n",
    "    train(model, train_dl, dev_dl, optimizer, scheduler, label2id, DEVICE)\n",
    "\n",
    "    best_path = os.path.join(OUT_DIR, \"best_model.pt\")\n",
    "    if os.path.exists(best_path):\n",
    "        model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n",
    "        print(\"\\nüß™ Final Test Evaluation\")\n",
    "        test_metrics = evaluate(model, test_dl, label2id, DEVICE)\n",
    "        print(f\"Final Test Macro-F1: {test_metrics['macro_f1']:.4f}, Acc: {test_metrics['accuracy']:.4f}\")\n",
    "    else:\n",
    "        print(\"No best model saved, skipping test eval.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5641a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: {'Comparison': 0, 'Contingency': 1, 'Expansion': 2, 'Temporal': 3}\n",
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1 (aux_w=0.0500): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2230/2230 [04:45<00:00,  7.80it/s, loss=1.0429]\n",
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Comparison     0.5104    0.5326    0.5213        92\n",
      " Contingency     0.7783    0.6798    0.7257       253\n",
      "   Expansion     0.6783    0.8241    0.7441       307\n",
      "    Temporal     0.5152    0.2394    0.3269        71\n",
      "\n",
      "    accuracy                         0.6791       723\n",
      "   macro avg     0.6205    0.5690    0.5795       723\n",
      "weighted avg     0.6759    0.6791    0.6684       723\n",
      "\n",
      "Epoch 1 dev macro-F1: 0.5795, acc: 0.6791\n",
      "‚úÖ Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2 (aux_w=0.0500): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2230/2230 [04:35<00:00,  8.11it/s, loss=0.8100]\n",
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Comparison     0.5934    0.5870    0.5902        92\n",
      " Contingency     0.7559    0.7589    0.7574       253\n",
      "   Expansion     0.7251    0.7818    0.7524       307\n",
      "    Temporal     0.6383    0.4225    0.5085        71\n",
      "\n",
      "    accuracy                         0.7137       723\n",
      "   macro avg     0.6782    0.6375    0.6521       723\n",
      "weighted avg     0.7106    0.7137    0.7095       723\n",
      "\n",
      "Epoch 2 dev macro-F1: 0.6521, acc: 0.7137\n",
      "‚úÖ Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3 (aux_w=0.0500): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2230/2230 [04:35<00:00,  8.08it/s, loss=0.7820]\n",
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Comparison     0.6173    0.5435    0.5780        92\n",
      " Contingency     0.7860    0.7115    0.7469       253\n",
      "   Expansion     0.7060    0.8371    0.7660       307\n",
      "    Temporal     0.6122    0.4225    0.5000        71\n",
      "\n",
      "    accuracy                         0.7151       723\n",
      "   macro avg     0.6804    0.6287    0.6477       723\n",
      "weighted avg     0.7135    0.7151    0.7093       723\n",
      "\n",
      "Epoch 3 dev macro-F1: 0.6477, acc: 0.7151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4 (aux_w=0.0500): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2230/2230 [04:35<00:00,  8.09it/s, loss=0.6152]\n",
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Comparison     0.6044    0.5978    0.6011        92\n",
      " Contingency     0.7705    0.7431    0.7565       253\n",
      "   Expansion     0.7188    0.8078    0.7607       307\n",
      "    Temporal     0.6512    0.3944    0.4912        71\n",
      "\n",
      "    accuracy                         0.7178       723\n",
      "   macro avg     0.6862    0.6358    0.6524       723\n",
      "weighted avg     0.7157    0.7178    0.7125       723\n",
      "\n",
      "Epoch 4 dev macro-F1: 0.6524, acc: 0.7178\n",
      "‚úÖ Saved best model.\n",
      "Training complete.\n",
      "\n",
      "üß™ Final Test Evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Comparison     0.5735    0.5652    0.5693       138\n",
      " Contingency     0.7476    0.6882    0.7167       340\n",
      "   Expansion     0.7290    0.7958    0.7610       480\n",
      "    Temporal     0.5405    0.3846    0.4494        52\n",
      "\n",
      "    accuracy                         0.7069      1010\n",
      "   macro avg     0.6477    0.6085    0.6241      1010\n",
      "weighted avg     0.7043    0.7069    0.7038      1010\n",
      "\n",
      "Final Test Macro-F1: 0.6241, Acc: 0.7069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mÂú®ÂΩìÂâçÂçïÂÖÉÊ†ºÊàñ‰∏ä‰∏Ä‰∏™ÂçïÂÖÉÊ†º‰∏≠ÊâßË°å‰ª£Á†ÅÊó∂ Kernel Â¥©Ê∫É„ÄÇ\n",
      "\u001b[1;31mËØ∑Êü•ÁúãÂçïÂÖÉÊ†º‰∏≠ÁöÑ‰ª£Á†ÅÔºå‰ª•Á°ÆÂÆöÊïÖÈöúÁöÑÂèØËÉΩÂéüÂõ†„ÄÇ\n",
      "\u001b[1;31mÂçïÂáª<a href='https://aka.ms/vscodeJupyterKernelCrash'>Ê≠§Â§Ñ</a>‰∫ÜËß£ËØ¶ÁªÜ‰ø°ÊÅØ„ÄÇ\n",
      "\u001b[1;31mÊúâÂÖ≥Êõ¥Â§öËØ¶ÁªÜ‰ø°ÊÅØÔºåËØ∑Êü•Áúã Jupyter <a href='command:jupyter.viewOutput'>log</a>„ÄÇ"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c3f9b1",
   "metadata": {},
   "source": [
    "ÂèÇÊï∞Ôºö\n",
    "SEED = 42\n",
    "EPOCHS = 4\n",
    "BATCH_SIZE = 8\n",
    "LR = 3e-5\n",
    "MAX_LEN = 256\n",
    "MODEL_NAME = \"../models/flan-t5-base\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DATA_DIR = \"../datasets/pdtb3_T5\"\n",
    "OUT_DIR = \"outputs_latent_idrr_denoise_Intersupervised\"\n",
    "LOOP_K = 3\n",
    "AUX_WEIGHT = 0.05\n",
    "NOISE_STD = 0.02\n",
    "LAMBDA_DENOISE = 0\n",
    "DENOISE_LAST_ONLY = True\n",
    "AUX_WARMUP_EPOCHS = 1  # üöÄ Âä®ÊÄÅÂêØÁî®‰∏≠Èó¥ÁõëÁù£\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "\n",
    "Labels: {'Comparison': 0, 'Contingency': 1, 'Expansion': 2, 'Temporal': 3}\n",
    "Device: cuda\n",
    "Train Epoch 1 (aux_w=0.0500): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2230/2230 [04:33<00:00,  8.14it/s, loss=1.0429]\n",
    "                                                     \n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "  Comparison     0.5104    0.5326    0.5213        92\n",
    " Contingency     0.7783    0.6798    0.7257       253\n",
    "   Expansion     0.6783    0.8241    0.7441       307\n",
    "    Temporal     0.5152    0.2394    0.3269        71\n",
    "\n",
    "    accuracy                         0.6791       723\n",
    "   macro avg     0.6205    0.5690    0.5795       723\n",
    "weighted avg     0.6759    0.6791    0.6684       723\n",
    "\n",
    "Epoch 1 dev macro-F1: 0.5795, acc: 0.6791\n",
    "‚úÖ Saved best model.\n",
    "Train Epoch 2 (aux_w=0.0500): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2230/2230 [04:34<00:00,  8.14it/s, loss=0.8100]\n",
    "                                                     \n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "  Comparison     0.5934    0.5870    0.5902        92\n",
    " Contingency     0.7559    0.7589    0.7574       253\n",
    "   Expansion     0.7251    0.7818    0.7524       307\n",
    "    Temporal     0.6383    0.4225    0.5085        71\n",
    "\n",
    "    accuracy                         0.7137       723\n",
    "   macro avg     0.6782    0.6375    0.6521       723\n",
    "weighted avg     0.7106    0.7137    0.7095       723\n",
    "\n",
    "Epoch 2 dev macro-F1: 0.6521, acc: 0.7137\n",
    "‚úÖ Saved best model.\n",
    "Train Epoch 3 (aux_w=0.0500): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2230/2230 [04:34<00:00,  8.11it/s, loss=0.7820]\n",
    "                                                     \n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "  Comparison     0.6173    0.5435    0.5780        92\n",
    " Contingency     0.7860    0.7115    0.7469       253\n",
    "   Expansion     0.7060    0.8371    0.7660       307\n",
    "    Temporal     0.6122    0.4225    0.5000        71\n",
    "\n",
    "    accuracy                         0.7151       723\n",
    "   macro avg     0.6804    0.6287    0.6477       723\n",
    "weighted avg     0.7135    0.7151    0.7093       723\n",
    "\n",
    "Epoch 3 dev macro-F1: 0.6477, acc: 0.7151\n",
    "Train Epoch 4 (aux_w=0.0500): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2230/2230 [04:34<00:00,  8.12it/s, loss=0.6152]\n",
    "                                                     \n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "  Comparison     0.6044    0.5978    0.6011        92\n",
    " Contingency     0.7705    0.7431    0.7565       253\n",
    "   Expansion     0.7188    0.8078    0.7607       307\n",
    "    Temporal     0.6512    0.3944    0.4912        71\n",
    "\n",
    "    accuracy                         0.7178       723\n",
    "   macro avg     0.6862    0.6358    0.6524       723\n",
    "weighted avg     0.7157    0.7178    0.7125       723\n",
    "\n",
    "Epoch 4 dev macro-F1: 0.6524, acc: 0.7178\n",
    "‚úÖ Saved best model.\n",
    "Training complete.\n",
    "\n",
    "üß™ Final Test Evaluation\n",
    "                                                       \n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "  Comparison     0.5735    0.5652    0.5693       138\n",
    " Contingency     0.7476    0.6882    0.7167       340\n",
    "   Expansion     0.7290    0.7958    0.7610       480\n",
    "    Temporal     0.5405    0.3846    0.4494        52\n",
    "\n",
    "    accuracy                         0.7069      1010\n",
    "   macro avg     0.6477    0.6085    0.6241      1010\n",
    "weighted avg     0.7043    0.7069    0.7038      1010\n",
    "\n",
    "Final Test Macro-F1: 0.6241, Acc: 0.7069\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (YYK_conda)",
   "language": "python",
   "name": "yyk_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
